---
title: "Metropolis Hastings & Gibbs Sampling"
author: "Sreenand Sasikumar"
date: "19/02/2020"
output: pdf_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)
```

# Question 1: Computations with Metropolis - Hastings

## 1.1

From the plot it is evident that the points do not converge nor resemble the target density distribution.
Also for few of the points the interval is large(flat) and hence can be said that there is no burn in period.
```{r, echo=FALSE}
target <- function(x)
{
  return(ifelse(x<0,0,(x^5 * exp(-x))))
}

mh_log <- function(Xo)
{
  X <- rep(0,10000)
  X[1] <- Xo
  
  for(i in 1:length(X))
  {
    Y <- rlnorm(1, meanlog = X[i], sdlog = 1)
    U <- runif(1)
    num <- target(Y) * dlnorm(X[i],Y,1)
    den <- target(X[i]) * dlnorm(Y,X[i],1)
    alpha <- min(1,(num / den))
    if (U < alpha)
    {
      X[i+1] <- Y
    }
    else
    {
      X[i+1] <- X[i]
    }
  }
  return(X)
}
set.seed(123456789)
X <- mh_log(3)
plot(X,type = 'l',col = "blue")
#hist(X)
hist(X,probability = TRUE, main="Histogram of values of x using LOG NORMAL as proposal density distribution")
# xx = seq(min(X),max(X),length=1000)
# lines(xx,target(xx),col="red")
# X.min
```



## 1.2

```{r,echo=FALSE}
mh_chi <- function(Xo)
{
  X <- rep(0,10000)
  X[1] <- Xo
  
  for(i in 1:(length(X)-1))
  {
    Y <- rchisq(1, floor(X[i] + 1))
    U <- runif(1)
    num <- target(Y) * dchisq(X[i],floor(X[i] + 1))
    den <- target(X[i]) * dchisq(Y,floor(Y + 1))
    alpha <- min(1,(num / den))
    if (U < alpha)
    {
      X[i+1] <- Y
    }
    else
    {
      X[i+1] <- X[i]
    }
  }
  return(X)
}
set.seed(123456789)
X <- mh_chi(2)
plot(X,type = 'l',col = "blue")
#hist(X)
hist(X,probability = TRUE, main="Histogram of values of x using CHI-SQUARE as proposal density distribution")
# xx = seq(min(X),max(X),length=100)
# lines(xx,target(xx),col="red")
```


## 1.3

By comparing the markov chain plots of 1.1 and 1.2, the chi square proposal density function does a good job in converging. We can see that the most of the points after initial interval, maintains a consistent pattern.
The burning period ends at the first few iterations and later a consistent pattern is followed towards convergence.

## 1.4

By using Gelam-Rubin method to analyse the sequence generated by chisquare distribution seems to converge since the Upper C.I  acheived is 1.


```{r,echo=FALSE}
library(coda)
mcmc_seq <- as.data.frame(matrix(NA,nrow = 10000,ncol = 10))
for(i in 1:10)
{
  mcmc_seq[,i] <- mh_chi(i)
}
mcmc_obj <- mcmc.list()
for(i in 1:10)
{
  mcmc_obj[[i]] <- as.mcmc(mcmc_seq[,i])
}
 
gelman.diag(mcmc_obj)
```


## 1.5

```{r,echo=FALSE}
cat("Mean of the Sample from step 1 is :")
set.seed(123456789)
mean(mh_log(2))

cat("Mean of the Sample from step 1 is :")
set.seed(123456789)
mean(mh_chi(2))
```

## 1.6

The original distribution of the Target distribution used is a form of Gamma Distribution.
By comparing the powers we can conclude that k = 5+1, hence it is a Gamma distribution with mu = 6.


# Question 2: Gibbs sampling

## 2.1

```{r,echo=FALSE}
load("chemical.RData")
chem_data <- as.data.frame(cbind(x=X,y=Y))
plot(chem_data$x,chem_data$y,xlab = "Day",ylab = "Concentration",xlim = c(0,50),ylim = c(0,2))
```

From the plot it is evident that, as the day is increasing the concentration of the chemical is also increasing. A linear model is not be a good fit but a higher degree polynomial can be a good fit for this particular dataset.

## 2.2

It is given that,
$$Y \sim\mathcal N(\mu_i,\sigma^2 = 0.2) $$

The formula for showing likelihood for $$p(\vec{Y} \mid \vec\mu)$$ found by taking likelihood of a density function of a normal distribution, that is,

$$\prod_{i=1}^{n} \frac{1}{\sqrt {2 \pi \sigma^2}}  {exp} {(Y_i - \mu)^2 / 2 \sigma^2}$$
$$p(\vec{Y} \mid \vec\mu) = (0.4\pi) ^ \frac {-n}{2} exp(\frac {-\sum_{i=1}^{n} (Y_i - \mu)^2} {0.4}) $$
The Prior $$p(\vec\mu)$$ is derived using the below chain rule,
$$p(\vec\mu) = p(\mu_1)p(\mu_2 |\mu_1)p(\mu_3 |\mu_2)...p(\mu_n |\mu_{n-1})$$
Therefore, by using the above chain rule we end up at the below formula for Prior,
$$p(\vec\mu)=(0.4\pi) ^ \frac {-(n-1)}{2} exp(\frac {-\sum_{i=2}^{n} (\mu_n - \mu_{n-1})^2} {0.4})$$

## 2.3

Using Bayes theorem, the Posterior Probability is given by the below formula,
$$p(\mu|Y) \propto exp-(\frac {(Y_1 - \mu_1)^2 + \sum_{i=2}^{n} (Y_i - \mu_i)^2+ (\mu_i - \mu_{i-1})^2} {2\sigma^2})$$

Now to find the distributions of $$(\mu_i | \vec \mu_{-i})$$ where $$(\vec \mu_{-i})$$ is a vector containing all $$\mu$$ values except of $$\mu_i$$

Let us consider a seperate formula for $$(\mu_1 | \vec \mu_{-1})$$ and it is said from Hint B that,
$$exp-(\frac {(x-a)^2 + (x-b)^2} {d}) \propto exp-(\frac {(x-(a+b)/2)^2)} {\frac {d}{2}})$$
Now, 
$$(\mu_1 | \vec \mu_{-1}) = exp-(\frac {(Y_1 - \mu_1)^2 + (\mu_2 - \mu_{1})^2} {2\sigma^2})$$
Considering the proportioanlity, below is the formula for $$(\mu_1 | \vec \mu_{-1})$$,
$$(\mu_1 | \vec \mu_{-1}) =exp-(\frac {(\mu_1-(Y_1+\mu_2)/2)^2)} {\frac {2\sigma^2}{2}}) $$

Similarly, using the same proportionality from Hint B, we can derive a formula for $$(\mu_n | \vec \mu_{-n})$$,
$$(\mu_n | \vec \mu_{-n}) =exp-(\frac {(\mu_n-(Y_n+\mu_{n-1})/2)^2)} {\frac {2\sigma^2}{2}}) $$

Formula for all remaining $$(\mu_i | \vec \mu_{-i})$$,let us consider the proportionality from Hint C,
$$exp-(\frac {(x-a)^2 + (x-b)^2 + (x-c)^2} {d}) \propto exp-(\frac {(x-(a+b+c)/3)^2)} {\frac {d}{3}})$$
For all the remanining $$(\mu_i | \vec \mu_{-i})$$,
$$(\mu_i | \vec \mu_{-i}) = exp-(\frac {(Y_i - \mu_i)^2 + (\mu_i - \mu_{i-1})^2 + (\mu_{i+1} - \mu_{i}) ^2} {2\sigma^2})$$
using the proportionality from Hint C, we can derive a formula for $$(\mu_i | \vec \mu_{-i})$$,
$$(\mu_i | \vec \mu_{-i}) = exp-(\frac {(\mu_i-(Y_i+\mu_{i-1} + \mu_{i+1})/3)^2)} {\frac {2\sigma^2}{3}})  $$

## 2.4

```{r, echo=FALSE}
mu <- matrix(0,nrow = 1000, ncol = 50)
variance <- 0.2
#RNGversion(min(as.character(getRversion()),"3.6.2")) 
# set.seed(1234567890, kind = "Mersenne-Twister", normal.kind = "Inversion")


for(iter in 1:5){

set.seed(12345)
for(i in 1:nrow(mu))
{
  for(j in 1:ncol(mu))
  {
    if(j==1)
    {
      mu[i,j] <- rnorm(1,mean = (Y[1] + mu[i,2])/2, sd = sqrt(variance/2))
    }
    else if(j == 50)
    {
      mu[i,j] <- rnorm(1,mean = (Y[j] + mu[i,j-1])/2, sd = sqrt(variance/2)) 
    }
    else
    {
      mu[i,j] <- rnorm(1,mean = (Y[j] + mu[i,j-1] + mu[i,j+1])/3, sd = sqrt(variance/3))
    }
  }
}

  
}
library(ggplot2)
chem_data$mu <- colMeans(mu)
plot(1:50,chem_data$mu,type = 'l',ylim = c(0,2.0),col="red",ylab = "mu",xlab = "1:50",panel.first=grid()) 
points(chem_data$x,chem_data$y,type = 'l',col='blue')
legend( "topleft",legend=c("Expected Mu sequence", "Original X,Y data"),
       col=c("red","blue"), lty=1:2, cex=0.8,
       title="Line types")






```

In the above plot, the blue line is the expected mu values calculated against the Day and Concentration of "chemical" dataset. It is evident that, the expected values have managed to have remove the noise and also have managed to catch the underlying dependence between Y and X.

## 2.5

```{r,echo=FALSE}
plot(mu[,50],type="l",col = "blue")
```
Above the Traceplot for mu_50 where in it is evident that the points have converged pretty good. We can also notice that there is slight fluctuation at the beginning, in this case is the burn period. Except for this burn period, the rest of the points maintain a consistent pattern.


### Appendix

```{r , ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```